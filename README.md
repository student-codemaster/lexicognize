# Lexicognize - AI Legal Text Summarization and Simplification

A comprehensive platform for fine-tuning and using AI models for legal text summarization and simplification. This project supports multiple state-of-the-art models and integrates with HuggingFace datasets, including the specialized multi_lexsum legal dataset.

## ğŸš€ Features

- **Multi-Model Support**: BART, PEGASUS, and Multilingual models
- **Dataset Integration**: Direct integration with HuggingFace datasets including multi_lexsum
- **User Authentication**: Secure user management and model access control
- **Training Pipeline**: Complete fine-tuning pipeline with evaluation metrics
- **RESTful API**: FastAPI-based API with comprehensive documentation
- **Data Processing**: Advanced text processing and validation for legal texts
- **Batch Inference**: Support for batch processing of multiple texts
- **Model Evaluation**: Built-in evaluation using ROUGE, BLEU, and other metrics

## ğŸ“Š Supported Datasets

The platform supports multiple datasets with a focus on legal text:

- **multi_lexsum** - Legal summarization with multiple summary lengths (long, short, tiny)
- **wikilarge** - Wikipedia simplification dataset
- **xsum** - Extreme summarization dataset
- **cnn_dailymail** - News article summarization
- **samsum** - Conversation summarization
- **legal_bench** - Legal NLP benchmark dataset
- **contract_nli** - Contract Natural Language Inference
- **eurlex** - Multi-lingual EU legislation

## ğŸ¤– Model Types

### BART (Bidirectional and Auto-Regressive Transformers)
- General purpose summarization
- Good for abstractive summarization
- Handles long documents well

### PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization)
- Specialized for abstractive summarization
- Excellent performance on summarization tasks
- Pre-trained on large-scale summarization datasets

### Multilingual Models
- Support for multiple languages
- Cross-lingual summarization capabilities
- Ideal for multi-jurisdictional legal documents

### Multi-Task Models
- Combined summarization and simplification
- Flexible task switching
- Efficient resource utilization

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended for training)
- PostgreSQL database (for production)

### Setup

1. **Clone the repository**
```bash
git clone <repository-url>
cd lexicognize
```

2. **Install dependencies**
```bash
pip install -r backend/requirements.txt
```

3. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your configuration
```

4. **Configure database**
Edit `backend/app/config.py` with your database settings.

5. **Run database migrations**
```bash
cd backend
alembic upgrade head
```

6. **Start the API server**
```bash
uvicorn backend.app.main:app --reload
```

7. **Access the API documentation**
Open http://localhost:8000/api/docs in your browser

## ğŸ“– Usage

### API Endpoints

#### Authentication
- `POST /api/auth/login` - User login
- `POST /api/auth/register` - User registration
- `GET /api/profile` - Get user profile

#### Datasets
- `GET /api/datasets/huggingface/available` - List available datasets
- `POST /api/datasets/huggingface/import` - Import dataset from HuggingFace
- `GET /api/datasets/` - List user datasets

#### Training
- `POST /api/training/start` - Start model training
- `GET /api/training/jobs` - List training jobs
- `GET /api/training/models` - List trained models

#### Inference
- `POST /api/inference/generate` - Generate summary
- `POST /api/inference/batch` - Batch generation
- `POST /api/inference/evaluate` - Evaluate model

### Example: Using multi_lexsum Dataset

1. **Import the dataset**
```python
import requests

response = requests.post(
    "http://localhost:8000/api/datasets/huggingface/import",
    json={
        "dataset_id": "multi_lexsum",
        "split": "train",
        "sample_size": 1000,
        "dataset_name": "legal_multi_lexsum"
    },
    headers={"Authorization": "Bearer <your-token>"}
)
```

2. **Start training**
```python
response = requests.post(
    "http://localhost:8000/api/training/start",
    json={
        "model_type": "bart",
        "dataset_id": <dataset-id>,
        "task": "summarization",
        "epochs": 3,
        "batch_size": 4
    },
    headers={"Authorization": "Bearer <your-token>"}
)
```

3. **Generate summaries**
```python
response = requests.post(
    "http://localhost:8000/api/inference/generate",
    json={
        "text": "Your legal text here...",
        "model_path": <model-path>,
        "model_type": "bart",
        "task": "summarization"
    },
    headers={"Authorization": "Bearer <your-token>"}
)
```

## ğŸ§ª Evaluation

The platform includes comprehensive evaluation metrics:

- **ROUGE Scores**: ROUGE-1, ROUGE-2, ROUGE-L
- **BLEU Score**: For translation quality assessment
- **Custom Metrics**: Task-specific evaluations
- **Model Comparison**: Side-by-side model performance

## ğŸ“ Project Structure

```
lexicognize/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ auth/           # Authentication and authorization
â”‚   â”‚   â”œâ”€â”€ database/       # Database models and sessions
â”‚   â”‚   â”œâ”€â”€ models/         # Model trainers (BART, PEGASUS, etc.)
â”‚   â”‚   â”œâ”€â”€ routes/         # API endpoints
â”‚   â”‚   â”œâ”€â”€ utils/          # Utilities and helpers
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration
â”‚   â”‚   â””â”€â”€ main.py         # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ frontend/              # React frontend (optional)
â”œâ”€â”€ data/                 # Data storage
â”œâ”€â”€ docker-compose.yaml   # Docker compose configuration
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Configuration

### Environment Variables

Key environment variables to configure:

```bash
# Database
DATABASE_URL=postgresql://user:password@localhost/lexicognize

# Security
SECRET_KEY=your-secret-key
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Model Storage
DATA_DIR=./data
MODEL_CACHE_DIR=./data/models

# API Settings
API_V1_STR=/api
PROJECT_NAME=Lexicognize
```

### Model Configuration

Models can be configured with various parameters:

- **Learning Rate**: 5e-5 (default)
- **Batch Size**: 4 (default, adjust based on GPU memory)
- **Epochs**: 3 (default)
- **Max Length**: 1024 (input), 256 (output)
- **Evaluation Strategy**: Steps-based evaluation

## ğŸš€ Deployment

### Docker Deployment

1. **Build and run with Docker Compose**
```bash
docker-compose up -d
```

2. **Scale services**
```bash
docker-compose up -d --scale api=3
```

### Production Deployment

For production deployment:

1. Use PostgreSQL database
2. Configure Redis for caching
3. Set up proper SSL/TLS
4. Configure monitoring and logging
5. Use GPU instances for model training

## ğŸ“ˆ Performance

### Benchmarks

Typical performance metrics on legal datasets:

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU |
|-------|---------|---------|---------|------|
| BART | 0.45 | 0.22 | 0.38 | 0.32 |
| PEGASUS | 0.48 | 0.25 | 0.41 | 0.35 |
| Multilingual | 0.42 | 0.20 | 0.36 | 0.30 |

### Hardware Requirements

- **Training**: NVIDIA GPU with 8GB+ VRAM
- **Inference**: CPU or GPU with 4GB+ memory
- **Storage**: 10GB+ for models and datasets

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:

- Create an issue on GitHub
- Check the API documentation at `/api/docs`
- Review the example notebooks and scripts

## ğŸ”® Roadmap

Future enhancements planned:

- [ ] Additional model architectures (T5, LED)
- [ ] Web-based frontend interface
- [ ] Advanced legal entity recognition
- [ ] Multi-document summarization
- [ ] Real-time inference streaming
- [ ] Model versioning and A/B testing
- [ ] Integration with legal document management systems

## ğŸ“š References

- [HuggingFace Transformers](https://huggingface.co/transformers/)
- [BART Paper](https://arxiv.org/abs/1910.13461)
- [PEGASUS Paper](https://arxiv.org/abs/1912.08777)
- [multi_lexsum Dataset](https://huggingface.co/datasets/allenai/multi_lexsum)